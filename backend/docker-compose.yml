services:
  # Frontend Service (Next.js)
  frontend:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_BACKEND_API_URL=http://localhost:8000
    networks:
      - basechat_network
    restart: unless-stopped
    depends_on:
      main-backend:
        condition: service_healthy

  # Main Backend Service (FastAPI + LangGraph)
  main-backend:
    build:
      context: ./main-backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./main-backend/app:/app/app
      - ./main-backend/alembic.ini:/app/alembic.ini
      - ./main-backend/alembic:/app/alembic
      - ./logs:/app/logs
    env_file:
      - env.example
    environment:
      - PYTHONPATH=/app
      - LLM_AGENT_URL=http://backend-llm-agent-openai-1:8001
      - MCP_SERVER_URL=http://backend-mcp-server-1:8002
      - DATABASE_URL=postgresql://admin:higk8156@basechat_postgres:5432/basechat
      - REDIS_URL=redis://basechat_redis:6379/0
      - DEBUG=true
      - SECRET_KEY=your-secret-key-change-in-production
    networks:
      - basechat_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      llm-agent-openai:
        condition: service_healthy

  # LLM Agent Service (OpenAI-based)
  llm-agent-openai:
    build:
      context: ./llm-agent
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    volumes:
      - ./llm-agent/app:/app/app
      - ./logs:/app/logs
    env_file:
      - env.example
    environment:
      - PYTHONPATH=/app
      - LLM_TYPE=openai
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      - REDIS_URL=redis://basechat_redis:6379/0
      - DEBUG=true
      - SECRET_KEY=your-secret-key-change-in-production
    networks:
      - basechat_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # LLM Agent Service (vLLM-based) - Placeholder for future implementation
  llm-agent-vllm:
    profiles: ["vllm"]
    build:
      context: ./llm-agent
      dockerfile: Dockerfile
    ports:
      - "8001:8001"
    volumes:
      - ./llm-agent/app:/app/app
      - ./logs:/app/logs
    environment:
      - PYTHONPATH=/app
      - LLM_TYPE=vllm
      - VLLM_MODEL=${VLLM_MODEL:-llama2-7b}
      - REDIS_URL=${REDIS_URL}
      - DEBUG=true
    networks:
      - basechat_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # MCP Server Service (FastAPI-MCP)
  mcp-server:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    ports:
      - "8002:8002"
    volumes:
      - ./mcp-server/app:/app/app
      - ./logs:/app/logs
    env_file:
      - env.example
    environment:
      - PYTHONPATH=/app
      - DATABASE_URL=postgresql://admin:higk8156@basechat_postgres:5432/basechat
      - REDIS_URL=redis://basechat_redis:6379/0
      - DEBUG=true
      - SECRET_KEY=your-secret-key-change-in-production
    networks:
      - basechat_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Embedding Server Service
  embedding-server:
    build:
      context: ./embedding-server
      dockerfile: Dockerfile
    ports:
      - "8003:8003"
    volumes:
      - ./embedding-server/app:/app/app
      - ./logs:/app/logs
    env_file:
      - env.example
    environment:
      - PYTHONPATH=/app
      - EMBEDDING_DATABASE_URL=postgresql://embedding_user:embedding_password@basechat_embedding_postgres:5432/embeddings
      - REDIS_URL=redis://basechat_redis:6379/1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      - DEBUG=true
    networks:
      - basechat_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Celery Worker Service
  celery-worker:
    build:
      context: ./embedding-server
      dockerfile: Dockerfile
    command: celery -A app.celery_app worker --loglevel=info
    volumes:
      - ./embedding-server/app:/app/app
      - ./logs:/app/logs
    env_file:
      - env.example
    environment:
      - PYTHONPATH=/app
      - EMBEDDING_DATABASE_URL=postgresql://embedding_user:embedding_password@basechat_embedding_postgres:5432/embeddings
      - REDIS_URL=redis://basechat_redis:6379/1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      - DEBUG=true
    networks:
      - basechat_network
    restart: unless-stopped

  # Celery Beat Service (for scheduled tasks)
  celery-beat:
    build:
      context: ./embedding-server
      dockerfile: Dockerfile
    command: celery -A app.celery_app beat --loglevel=info
    volumes:
      - ./embedding-server/app:/app/app
      - ./logs:/app/logs
    env_file:
      - env.example
    environment:
      - PYTHONPATH=/app
      - EMBEDDING_DATABASE_URL=postgresql://embedding_user:embedding_password@basechat_embedding_postgres:5432/embeddings
      - REDIS_URL=redis://basechat_redis:6379/1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      - DEBUG=true
    networks:
      - basechat_network
    restart: unless-stopped


  # Note: Nginx is now managed in system-docker/docker-compose.yml
  # This service has been moved to system-docker for better organization

networks:
  basechat_network:
    external: true
    name: basechat_network 